{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Inference with Weave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weave Official Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import asyncio\n",
    "import weave\n",
    "from weave.flow.scorer import MultiTaskBinaryClassificationF1\n",
    "import openai\n",
    "\n",
    "# We create a model class with one predict function.\n",
    "# All inputs, predictions and parameters are automatically captured for easy inspection.\n",
    "\n",
    "class ExtractFruitsModel(weave.Model):\n",
    "    model_name: str\n",
    "    prompt_template: str\n",
    "\n",
    "    @weave.op()\n",
    "    async def predict(self, sentence: str) -> dict:\n",
    "        client = openai.AsyncClient()\n",
    "\n",
    "        response = await client.chat.completions.create(\n",
    "            model=self.model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": self.prompt_template.format(sentence=sentence)}\n",
    "            ],\n",
    "            response_format={ \"type\": \"json_object\" }\n",
    "        )\n",
    "        result = response.choices[0].message.content\n",
    "        if result is None:\n",
    "            raise ValueError(\"No response from model\")\n",
    "        parsed = json.loads(result)\n",
    "        return parsed\n",
    "\n",
    "# We call init to begin capturing data in the project, intro-example.\n",
    "weave.init('intro-example')\n",
    "\n",
    "# We create our model with our system prompt.\n",
    "model = ExtractFruitsModel(name='gpt4',\n",
    "                           model_name='gpt-4-0125-preview',\n",
    "                           prompt_template='Extract fields (\"fruit\": <str>, \"color\": <str>, \"flavor\") from the following text, as json: {sentence}')\n",
    "sentences = [\"There are many fruits that were found on the recently discovered planet Goocrux. There are neoskizzles that grow there, which are purple and taste like candy.\",\n",
    "\"Pounits are a bright green color and are more savory than sweet.\",\n",
    "\"Finally, there are fruits called glowls, which have a very sour and bitter taste which is acidic and caustic, and a pale orange tinge to them.\"]\n",
    "labels = [\n",
    "    {'fruit': 'neoskizzles', 'color': 'purple', 'flavor': 'candy'},\n",
    "    {'fruit': 'pounits', 'color': 'bright green', 'flavor': 'savory'},\n",
    "    {'fruit': 'glowls', 'color': 'pale orange', 'flavor': 'sour and bitter'}\n",
    "]\n",
    "examples = [\n",
    "    {'id': '0', 'sentence': sentences[0], 'target': labels[0]},\n",
    "    {'id': '1', 'sentence': sentences[1], 'target': labels[1]},\n",
    "    {'id': '2', 'sentence': sentences[2], 'target': labels[2]}\n",
    "]\n",
    "# If you have already published the Dataset, you can run:\n",
    "# dataset = weave.ref('example_labels').get()\n",
    "\n",
    "# We define a scoring functions to compare our model predictions with a ground truth label.\n",
    "@weave.op()\n",
    "def fruit_name_score(target: dict, model_output: dict) -> dict:\n",
    "    return {'correct': target['fruit'] == model_output['fruit']}\n",
    "\n",
    "# Finally, we run an evaluation of this model.\n",
    "# This will generate a prediction for each input example, and then score it with each scoring function.\n",
    "evaluation = weave.Evaluation(\n",
    "    name='fruit_eval',\n",
    "    dataset=examples, scorers=[MultiTaskBinaryClassificationF1(class_names=[\"fruit\", \"color\", \"flavor\"]), fruit_name_score],\n",
    ")\n",
    "print(asyncio.run(evaluation.evaluate(model)))\n",
    "# if you're in a Jupyter Notebook, run:\n",
    "# await evaluation.evaluate(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 8700\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1088\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1088\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_dataset = load_dataset(\"MAdAiLab/twitter_disaster\")\n",
    "twitter_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': '@sabcnewsroom sabotage!I rule out structural failure', 'label': 0},\n",
       " {'text': 'Two giant cranes holding a bridge collapse into nearby homes http://t.co/UmANaaHwMI',\n",
       "  'label': 1},\n",
       " {'text': '@yeetrpan I asked if they were hiring and they said not you I was devastated.',\n",
       "  'label': 0},\n",
       " {'text': 'Watch This Airport Get Swallowed Up By A Sandstorm In Under A Minute http://t.co/7IJlZ6BcSP',\n",
       "  'label': 1},\n",
       " {'text': 'Survived my first #tubestrike thanks to @Citymapper', 'label': 0}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = twitter_dataset['train'].to_list()[:5]\n",
    "examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intergrating Weave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in as Weights & Biases user: akshat_patil.\n",
      "View Weave data at https://wandb.ai/madailab/seq-clf-vllm-inference/weave\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import weave\n",
    "# import asyncio\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "WANDB_API_KEY = os.getenv(\"WANDB_API_KEY\")\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "from langchain_core.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "\n",
    "weave.init('seq-clf-vllm-inference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 8700\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1088\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1088\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_dataset = load_dataset(\"MAdAiLab/twitter_disaster\")\n",
    "twitter_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harpreet_guest2/akshat/LLM-Inference/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "2024-05-13 00:07:56,225\tINFO worker.py:1749 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-13 00:07:56 llm_engine.py:100] Initializing an LLM engine (v0.4.2) with config: model='meta-llama/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=meta-llama/Meta-Llama-3-8B-Instruct)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-13 00:07:59 utils.py:660] Found nccl from library /home/harpreet_guest2/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "\u001b[36m(RayWorkerWrapper pid=425395)\u001b[0m INFO 05-13 00:07:59 utils.py:660] Found nccl from library /home/harpreet_guest2/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "INFO 05-13 00:08:00 selector.py:81] Cannot use FlashAttention-2 backend because the flash_attn package is not found. Please install it for better performance.\n",
      "INFO 05-13 00:08:00 selector.py:32] Using XFormers backend.\n",
      "\u001b[36m(RayWorkerWrapper pid=425395)\u001b[0m INFO 05-13 00:08:00 selector.py:81] Cannot use FlashAttention-2 backend because the flash_attn package is not found. Please install it for better performance.\n",
      "\u001b[36m(RayWorkerWrapper pid=425395)\u001b[0m INFO 05-13 00:08:00 selector.py:32] Using XFormers backend.\n",
      "INFO 05-13 00:08:01 pynccl_utils.py:43] vLLM is using nccl==2.18.1\n",
      "\u001b[36m(RayWorkerWrapper pid=425395)\u001b[0m INFO 05-13 00:08:01 pynccl_utils.py:43] vLLM is using nccl==2.18.1\n",
      "INFO 05-13 00:08:02 utils.py:132] reading GPU P2P access cache from /home/harpreet_guest2/.config/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[36m(RayWorkerWrapper pid=425395)\u001b[0m INFO 05-13 00:08:02 utils.py:132] reading GPU P2P access cache from /home/harpreet_guest2/.config/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "INFO 05-13 00:08:02 weight_utils.py:199] Using model weights format ['*.safetensors']\n",
      "\u001b[36m(RayWorkerWrapper pid=425395)\u001b[0m INFO 05-13 00:08:03 weight_utils.py:199] Using model weights format ['*.safetensors']\n",
      "INFO 05-13 00:08:04 model_runner.py:175] Loading model weights took 7.4829 GB\n",
      "\u001b[36m(RayWorkerWrapper pid=425395)\u001b[0m INFO 05-13 00:08:05 model_runner.py:175] Loading model weights took 7.4829 GB\n",
      "INFO 05-13 00:08:06 distributed_gpu_executor.py:45] # GPU blocks: 38400, # CPU blocks: 4096\n",
      "INFO 05-13 00:08:08 block_manager_v1.py:246] Automatic prefix caching is enabled.\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(\n",
    "    model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    tensor_parallel_size=2,\n",
    "    trust_remote_code=True,\n",
    "    enforce_eager=True,\n",
    "    gpu_memory_utilization=0.99,\n",
    "    enable_prefix_caching=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = twitter_dataset['train'].to_list()[:5]\n",
    "# example_prompt = PromptTemplate(\n",
    "#     input_variables=[\"text\", \"label\"], template=\"Text: {text} \\nClassification Label: {label}\"\n",
    "# )\n",
    "# prompt = FewShotPromptTemplate(\n",
    "#     examples=examples,\n",
    "#     example_prompt=example_prompt,\n",
    "#     suffix=\"Text: {text} \\nClassification Label: \",\n",
    "#     input_variables=[\"text\"],\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '@sabcnewsroom sabotage!I rule out structural failure', 'label': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"Text: @sabcnewsroom sabotage!I rule out structural failure \n",
    "Classification Label: 0\n",
    "\n",
    "Text: Two giant cranes holding a bridge collapse into nearby homes http://t.co/UmANaaHwMI \n",
    "Classification Label: 1\n",
    "\n",
    "Text: @yeetrpan I asked if they were hiring and they said not you I was devastated. \n",
    "Classification Label: 0\n",
    "\n",
    "Text: Watch This Airport Get Swallowed Up By A Sandstorm In Under A Minute http://t.co/7IJlZ6BcSP \n",
    "Classification Label: 1\n",
    "\n",
    "Text: Survived my first #tubestrike thanks to @Citymapper \n",
    "Classification Label: 0\n",
    "\n",
    "Text: {text} \n",
    "Classification Label: \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"\n",
    "Given the following tweet:\n",
    "\n",
    "\"{text}\"\n",
    "\n",
    "0: negative\n",
    "1: positive\n",
    "\n",
    "What is your answer? Please respond with 0 or 1.\n",
    "\n",
    "Answer: \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceClassificationModel(weave.Model):\n",
    "    model_name: str\n",
    "    prompt_template: str\n",
    "    llm: LLM\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, texts: list[str]) -> list[int]:\n",
    "        sampling_params = SamplingParams(temperature=0, max_tokens=1)\n",
    "        prompt_texts = [self.prompt_template.format(text=text) for text in texts]\n",
    "        outputs = self.llm.generate(prompt_texts, sampling_params)\n",
    "        predicted_labels = []\n",
    "        for output in outputs:\n",
    "            try:\n",
    "                predicted_labels.append(int(output.outputs[0].text))\n",
    "            except ValueError:\n",
    "                predicted_labels.append(-1)\n",
    "        return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SequenceClassificationModel(\n",
    "    name='twitter-zero-shot-classification',\n",
    "    model_name='meta-llama/Meta-Llama-3-70B-Instruct',\n",
    "    prompt_template=prompt,\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = twitter_dataset['test']\n",
    "test_examples = [\n",
    "    {'id': str(i), 'text': text, 'label': label}\n",
    "    for i, (text, label) in enumerate(zip(test_inputs['text'], test_inputs['label']))\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Providence Health &amp; Services: Emergency Services Supervisor - Emergency Department... (#Kodiak AK) http://t.co/AQcSUSqbDy #Healthcare #Job'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_examples[5]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '0',\n",
       "  'text': 'Heavy Rainfall and Flooding in Northern #VietNam | Situation Report No.2 http://t.co/hVxu1Zcvau http://t.co/iJmCCMHh5G',\n",
       "  'label': 1},\n",
       " {'id': '1',\n",
       "  'text': 'Bolshevik government monopolized food supply to seize power over hunhry population. Artificial famine was the result https://t.co/0xOUv7DHWz',\n",
       "  'label': 1},\n",
       " {'id': '2',\n",
       "  'text': 'WHELEN MODEL 295SS-100 SIREN AMPLIFIER POLICE EMERGENCY VEHICLE - Full read by eBay http://t.co/UGR6REFZpT http://t.co/eYyUqX4Tbt',\n",
       "  'label': 0},\n",
       " {'id': '3',\n",
       "  'text': '#Autoinsurance industry clueless on driverless cars : #healthinsurance http://t.co/YdEtWgRibk',\n",
       "  'label': 1},\n",
       " {'id': '4',\n",
       "  'text': 'Gunmen kill four in El Salvador bus attack: Suspected Salvadoran gang members killed four people and wounded s... http://t.co/r8k6rXw6D6',\n",
       "  'label': 1}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_examples[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1088/1088 [00:05<00:00, 206.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🍩 https://wandb.ai/madailab/seq-clf-vllm-inference/r/call/6bfa769e-3f55-4ccd-aa38-227d80741c8a\n",
      "Accuracy: 0.4071691176470588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "@weave.op()\n",
    "def evaluate_model(model: SequenceClassificationModel, test_examples: list) -> dict:\n",
    "    texts = [ex['text'] for ex in test_examples]\n",
    "    y_true = [ex['label'] for ex in test_examples]\n",
    "    y_pred = model.predict(texts)\n",
    "    valid_indices = [i for i, pred in enumerate(y_pred) if pred != -1]\n",
    "    accuracy = accuracy_score([y_true[i] for i in valid_indices], [y_pred[i] for i in valid_indices])\n",
    "    return {'accuracy': accuracy}\n",
    "\n",
    "results = evaluate_model(model, test_examples)\n",
    "print(\"Accuracy:\", results['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.4071691176470588}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
